   | Name                                                  | Type                          | Params
---------------------------------------------------------------------------------------------------------
0  | train_metrics                                         | ModuleDict                    | 0     
1  | train_metrics.f1                                      | F1                            | 0     
2  | val_metrics                                           | ModuleDict                    | 0     
3  | val_metrics.f1                                        | F1                            | 0     
4  | model                                                 | BertForSequenceClassification | 4.4 M 
5  | model.bert                                            | BertModel                     | 4.4 M 
6  | model.bert.embeddings                                 | BertEmbeddings                | 4.0 M 
7  | model.bert.embeddings.word_embeddings                 | Embedding                     | 3.9 M 
8  | model.bert.embeddings.position_embeddings             | Embedding                     | 65.5 K
9  | model.bert.embeddings.token_type_embeddings           | Embedding                     | 256   
10 | model.bert.embeddings.LayerNorm                       | LayerNorm                     | 256   
11 | model.bert.embeddings.dropout                         | Dropout                       | 0     
12 | model.bert.encoder                                    | BertEncoder                   | 396 K 
13 | model.bert.encoder.layer                              | ModuleList                    | 396 K 
14 | model.bert.encoder.layer.0                            | BertLayer                     | 198 K 
15 | model.bert.encoder.layer.0.attention                  | BertAttention                 | 66.3 K
16 | model.bert.encoder.layer.0.attention.self             | BertSelfAttention             | 49.5 K
17 | model.bert.encoder.layer.0.attention.self.query       | Linear                        | 16.5 K
18 | model.bert.encoder.layer.0.attention.self.key         | Linear                        | 16.5 K
19 | model.bert.encoder.layer.0.attention.self.value       | Linear                        | 16.5 K
20 | model.bert.encoder.layer.0.attention.self.dropout     | Dropout                       | 0     
21 | model.bert.encoder.layer.0.attention.output           | BertSelfOutput                | 16.8 K
22 | model.bert.encoder.layer.0.attention.output.dense     | Linear                        | 16.5 K
23 | model.bert.encoder.layer.0.attention.output.LayerNorm | LayerNorm                     | 256   
24 | model.bert.encoder.layer.0.attention.output.dropout   | Dropout                       | 0     
25 | model.bert.encoder.layer.0.intermediate               | BertIntermediate              | 66.0 K
26 | model.bert.encoder.layer.0.intermediate.dense         | Linear                        | 66.0 K
27 | model.bert.encoder.layer.0.output                     | BertOutput                    | 65.9 K
28 | model.bert.encoder.layer.0.output.dense               | Linear                        | 65.7 K
29 | model.bert.encoder.layer.0.output.LayerNorm           | LayerNorm                     | 256   
30 | model.bert.encoder.layer.0.output.dropout             | Dropout                       | 0     
31 | model.bert.encoder.layer.1                            | BertLayer                     | 198 K 
32 | model.bert.encoder.layer.1.attention                  | BertAttention                 | 66.3 K
33 | model.bert.encoder.layer.1.attention.self             | BertSelfAttention             | 49.5 K
34 | model.bert.encoder.layer.1.attention.self.query       | Linear                        | 16.5 K
35 | model.bert.encoder.layer.1.attention.self.key         | Linear                        | 16.5 K
36 | model.bert.encoder.layer.1.attention.self.value       | Linear                        | 16.5 K
37 | model.bert.encoder.layer.1.attention.self.dropout     | Dropout                       | 0     
38 | model.bert.encoder.layer.1.attention.output           | BertSelfOutput                | 16.8 K
39 | model.bert.encoder.layer.1.attention.output.dense     | Linear                        | 16.5 K
40 | model.bert.encoder.layer.1.attention.output.LayerNorm | LayerNorm                     | 256   
41 | model.bert.encoder.layer.1.attention.output.dropout   | Dropout                       | 0     
42 | model.bert.encoder.layer.1.intermediate               | BertIntermediate              | 66.0 K
43 | model.bert.encoder.layer.1.intermediate.dense         | Linear                        | 66.0 K
44 | model.bert.encoder.layer.1.output                     | BertOutput                    | 65.9 K
45 | model.bert.encoder.layer.1.output.dense               | Linear                        | 65.7 K
46 | model.bert.encoder.layer.1.output.LayerNorm           | LayerNorm                     | 256   
47 | model.bert.encoder.layer.1.output.dropout             | Dropout                       | 0     
48 | model.bert.pooler                                     | BertPooler                    | 16.5 K
49 | model.bert.pooler.dense                               | Linear                        | 16.5 K
50 | model.bert.pooler.activation                          | Tanh                          | 0     
51 | model.dropout                                         | Dropout                       | 0     
52 | model.classifier                                      | Linear                        | 258   
---------------------------------------------------------------------------------------------------------
258       Trainable params
4.4 M     Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)